{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12_yLgZNUnekO4UBad6BJsceZipO-vRX0","timestamp":1728821928056}],"collapsed_sections":["HUKIfol-0WB7"],"authorship_tag":"ABX9TyOkcWsIfb6sitX12Lha3VHk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## BDA\n"],"metadata":{"id":"HUKIfol-0WB7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ms2Ob5EE1qp"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import scipy\n","import os\n","\n","import matplotlib.pyplot as plt\n","\n","\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import ensemble\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import brier_score_loss, precision_score, recall_score,f1_score, roc_auc_score, accuracy_score\n","from sklearn.metrics import confusion_matrix, roc_curve\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.cluster import KMeans\n","\n","import random\n","\n","from scipy.stats import ttest_ind"]},{"cell_type":"code","source":["#accessing files from google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OJVMS1bUE4-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"id":"qb2XPVyXFX9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder \\\n","  .appName('2.1. Google Cloud Storage (CSV) & Spark DataFrames') \\\n","  .getOrCreate()"],"metadata":{"id":"VOuHl_S7FqgZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading the CSV file\n","features = spark.read.csv('/content/drive/MyDrive/Big_data/Features_pd1.csv', header=True, inferSchema=True)\n","\n","# Print column names\n","print(features.columns)\n"],"metadata":{"id":"9KjVPJVWGenr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features"],"metadata":{"id":"fplopTmYG843"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features[features['is_fraud']==1].count()"],"metadata":{"id":"gb-6EDCdH9v8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FeaturesAll=features"],"metadata":{"id":"pTPBtv0XIJzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import log10, col\n","\n","FeaturesAll = FeaturesAll.withColumn('sum_tot_drug_cst', log10(col('sum_tot_drug_cst') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('sum_tot_clms', log10(col('sum_tot_clms') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('sum_tot_day_suply', log10(col('sum_tot_day_suply') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('Total_Payment_Sum', log10(col('Total_Payment_Sum') + 1.0))\n","\n","FeaturesAll = FeaturesAll.withColumn('avg_tot_drug_cst', log10(col('avg_tot_drug_cst') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('avg_tot_clms', log10(col('avg_tot_clms') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('avg_tot_day_suply', log10(col('avg_tot_day_suply') + 1.0))\n","\n","FeaturesAll = FeaturesAll.withColumn('max_tot_drug_cst', log10(col('max_tot_drug_cst') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('max_tot_clms', log10(col('max_tot_clms') + 1.0))\n","FeaturesAll = FeaturesAll.withColumn('max_tot_day_suply', log10(col('max_tot_day_suply') + 1.0))\n","\n","FeaturesAll = FeaturesAll.withColumn('claim_max-mean', col('max_tot_clms') - col('avg_tot_clms'))\n","FeaturesAll = FeaturesAll.withColumn('supply_max-mean', col('max_tot_day_suply') - col('avg_tot_day_suply'))\n","FeaturesAll = FeaturesAll.withColumn('drug_max-mean', col('max_tot_drug_cst') - col('avg_tot_drug_cst'))"],"metadata":{"id":"5NdUEVSfITKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FeaturesAll"],"metadata":{"id":"EOMQBzthIV0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","FeaturesAll = FeaturesAll.withColumn(\"Prscrbr_NPI\", col(\"Prscrbr_NPI\").cast(\"string\"))"],"metadata":{"id":"uDEfx4w2IpfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StringType\n","\n","categorical_features = ['Prscrbr_NPI', 'last_name', 'Speciality', 'first_name', 'city', 'state']\n","\n","for feature in categorical_features:\n","    FeaturesAll = FeaturesAll.withColumn(feature, FeaturesAll[feature].cast(StringType()))"],"metadata":{"id":"f888O0fkItGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numerical_features = ['sum_tot_drug_cst', 'avg_tot_drug_cst','Total_Payment_Sum',\n","       'max_tot_drug_cst', 'sum_tot_clms',\n","       'avg_tot_clms', 'max_tot_clms',\n","       'sum_tot_day_suply', 'avg_tot_day_suply', 'max_tot_day_suply',\n","    'claim_max-mean','supply_max-mean', 'drug_max-mean']"],"metadata":{"id":"pachwhlRIuuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target = ['is_fraud']"],"metadata":{"id":"Wwb6Y09zIxJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["allvars = categorical_features + numerical_features + target"],"metadata":{"id":"yyC7qwJZIzNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = FeaturesAll.select(\"is_fraud\").rdd.flatMap(lambda x: x).collect()\n","X = FeaturesAll.select([col(c) for c in allvars if c != 'is_fraud'])"],"metadata":{"id":"7eWXnvn3I1uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scikit learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","#from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_curve, auc"],"metadata":{"id":"6-w1UBz4I3Ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import DoubleType\n","from pyspark.ml.tuning import TrainValidationSplit\n","\n","# select the numerical columns from the original dataframe\n","numerical_features = ['sum_tot_drug_cst', 'avg_tot_drug_cst','Total_Payment_Sum',\n","       'max_tot_drug_cst', 'sum_tot_clms',\n","       'avg_tot_clms', 'max_tot_clms',\n","       'sum_tot_day_suply', 'avg_tot_day_suply', 'max_tot_day_suply',\n","    'claim_max-mean','supply_max-mean', 'drug_max-mean']\n","X = FeaturesAll.select(numerical_features)\n","\n","# convert numerical columns to double type\n","for feature in numerical_features:\n","    X = X.withColumn(feature, col(feature).cast(DoubleType()))\n","\n","# combine features into a single vector column\n","vectorAssembler = VectorAssembler(inputCols=X.columns, outputCol=\"features_vec\")\n","X = vectorAssembler.transform(X)\n","\n","# split the data into train and validation sets\n","train, test = X.randomSplit([0.8, 0.2], seed=0)\n","\n","# select the correct columns for input and output\n","X_train = train.select(X.columns)\n","X_valid = test.select(X.columns)\n","y_train = train.select(\"Total_Payment_Sum\")\n","y_valid = test.select(\"Total_Payment_Sum\")"],"metadata":{"id":"fjOYXnAxI709"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# fill null values in numerical columns with 0\n","for feature in numerical_features:\n","    X_train = X_train.withColumn(feature, col(feature).cast(\"double\"))\n","    X_valid = X_valid.withColumn(feature, col(feature).cast(\"double\"))\n","    X_train = X_train.na.fill(0, [feature])\n","    X_valid = X_valid.na.fill(0, [feature])\n","\n","# fill null values in categorical columns with 'NA'\n","for feature in categorical_features:\n","    if feature in X_train.columns:\n","        X_train = X_train.na.fill('NA', [feature])\n","    if feature in X_valid.columns:\n","        X_valid = X_valid.na.fill('NA', [feature])"],"metadata":{"id":"3u2B7EJIJZil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","X_train.select([col(col_name).cast(\"double\").alias(col_name) for col_name in numerical_features]).dtypes"],"metadata":{"id":"YjyzV-MTJgQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import rand\n","\n","df_len = FeaturesAll.count()\n","train_len = int(df_len * 0.8)\n","\n","df_train = FeaturesAll.orderBy(rand()).limit(train_len)\n","df_valid = FeaturesAll.orderBy(rand()).exceptAll(df_train)\n","\n","print(df_train.count())\n","print(df_valid.count())\n"],"metadata":{"id":"ghD9sKScJiRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.printSchema()"],"metadata":{"id":"AQ_UhyJ6JpiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["partD_Drug_df = spark.read.csv('/content/drive/MyDrive/Big_data/partD_Drugdf.csv', header=True, inferSchema=True)\n"],"metadata":{"id":"HCQHu39yJ89l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","partD_drug_train = partD_Drug_df.join(df_train.select('Prscrbr_NPI', 'is_fraud'), on=['Prscrbr_NPI'], how='inner')\n","partD_drug_all = partD_Drug_df.join(FeaturesAll.select('Prscrbr_NPI', 'is_fraud'), on=['Prscrbr_NPI'], how='inner')"],"metadata":{"id":"jg0Omtu5vtAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(partD_drug_train.filter(col(\"is_fraud\") == 1).count())"],"metadata":{"id":"IqQir4xuv2es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total records in train set\n","print(\"Total records in train set : \")\n","print(partD_drug_train.count())\n","\n","# Total Fraud in train set\n","print(\"Total Fraud in train set : \")\n","print(partD_drug_train.filter(\"is_fraud == 1\").count())\n","\n","# Show DataFrame\n","partD_drug_train.show()"],"metadata":{"id":"HvD9k2sLv6-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["partD_drug_train_20= partD_drug_train.sample(fraction=0.05, seed=42)"],"metadata":{"id":"Q0no5bygv9gK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_cols = ['Tot_Drug_Cst', 'Tot_Clms', 'Tot_Day_Suply']\n","\n","# Create a vector assembler to assemble the features into a vector\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# Apply the vector assembler to the training data\n","train_data = assembler.transform(partD_drug_train_20).select(\"features\", \"is_fraud\")\n","\n","# Split the data into training and test sets\n","train_set, test_set = train_data.randomSplit([0.7, 0.3], seed=12345)"],"metadata":{"id":"vkLu5f64xeaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_train_data = train_set.count()\n","print(\"Number of data in train_set:\", num_train_data)"],"metadata":{"id":"YDQYfg_RxiLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total records in train set\n","print(\"Number of data in train_set:\", num_train_data)\n","\n","num_test_data = test_set.count()\n","print(\"Number of data in test_set:\", num_test_data)"],"metadata":{"id":"zWZSBRHgxkIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total Fraud in train set\n","print(\"Total Fraud in train set : \")\n","print(train_set.filter(\"is_fraud == 1\").count())\n","\n","print(\"Total Fraud in test set : \")\n","print(test_set.filter(\"is_fraud == 1\").count())"],"metadata":{"id":"c0MsoatC0My4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total Fraud in train set\n","print(\"Total Non Fraud in train set : \")\n","print(train_set.filter(\"is_fraud == 0\").count())\n","\n","print(\"Total Non Fraud in test set : \")\n","print(test_set.filter(\"is_fraud == 0\").count())"],"metadata":{"id":"7OTrZIin4Vjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time"],"metadata":{"id":"SBM9WalhzPYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","\n","# Create a Naive Bayes model with default parameters\n","nb = NaiveBayes(featuresCol='features', labelCol='is_fraud')\n","\n","# Train the model using the training set\n","start = time.time()\n","nb_model = nb.fit(train_set)\n","end = time.time()\n","print(f\"Time to train Naive Bayes model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = nb_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")"],"metadata":{"id":"bZkhQgQHzZUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HtHDTgbrzeM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Class Balancing"],"metadata":{"id":"CO0tf1-o-jgF"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","from pyspark.ml.feature import VectorAssembler\n","\n","# Count total fraud and non-fraud cases\n","total_fraud = partD_drug_train.filter(col(\"is_fraud\") == 1).count()\n","total_non_fraud = partD_drug_train.filter(col(\"is_fraud\") == 0).count()\n","\n","print(\"Total Fraud cases:\", total_fraud)  # Should be 5216\n","print(\"Total Non-Fraud cases:\", total_non_fraud)  # Should be 20695433 - 5216\n","\n","# Sample the non-fraud cases to be exactly 2 times the number of fraud cases\n","target_non_fraud_count = 2 * total_fraud\n","partD_drug_train_non_fraud_sampled = partD_drug_train.filter(col(\"is_fraud\") == 0).sample(fraction=target_non_fraud_count / total_non_fraud, seed=42).limit(target_non_fraud_count)\n","\n","# Combine the sampled non-fraud cases and all fraud cases\n","balanced_data = partD_drug_train_non_fraud_sampled.union(partD_drug_train.filter(col(\"is_fraud\") == 1))\n","\n","# Assemble features for the balanced dataset\n","feature_cols = ['Tot_Drug_Cst', 'Tot_Clms', 'Tot_Day_Suply']\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","balanced_data_transformed = assembler.transform(balanced_data).select(\"features\", \"is_fraud\")\n","\n","# Split into training and test sets (70% train, 30% test)\n","train_set, test_set = balanced_data_transformed.randomSplit([0.7, 0.3], seed=12345)\n","\n","# Count the number of fraud and non-fraud cases in the balanced train and test sets\n","num_train_data = train_set.count()\n","num_test_data = test_set.count()\n","\n","num_fraud_train = train_set.filter(\"is_fraud == 1\").count()\n","num_non_fraud_train = train_set.filter(\"is_fraud == 0\").count()\n","\n","num_fraud_test = test_set.filter(\"is_fraud == 1\").count()\n","num_non_fraud_test = test_set.filter(\"is_fraud == 0\").count()\n","\n","# Print results\n","print(\"Number of data in train_set:\", num_train_data)\n","print(\"Number of data in test_set:\", num_test_data)\n","print(\"Total Fraud in train set:\", num_fraud_train)\n","print(\"Total Non-Fraud in train set:\", num_non_fraud_train)\n","print(\"Total Fraud in test set:\", num_fraud_test)\n","print(\"Total Non-Fraud in test set:\", num_non_fraud_test)\n","\n","# Assert that non-fraud cases are exactly 2 times the fraud cases\n","assert num_non_fraud_train == 2 * num_fraud_train, \"Non-fraud cases are not exactly 2 times fraud cases in train set\"\n","assert num_non_fraud_test == 2 * num_fraud_test, \"Non-fraud cases are not exactly 2 times fraud cases in test set\"\n"],"metadata":{"id":"IsL3YMnE-mP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_path = \"/content/drive/MyDrive/balanced_data2c.csv\"  # Change this to your desired path\n","balanced_data.coalesce(1).write.csv(output_path, header=True)"],"metadata":{"id":"VwjLD5feB02N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from pyspark.sql import functions as F\n","\n","# Create a logistic regression model with default parameters\n","lr = LogisticRegression(featuresCol='features', labelCol='is_fraud')\n","\n","# Train the model using the training set\n","start = time.time()\n","lr_model = lr.fit(train_set)\n","end = time.time()\n","print(f\"Time to train logistic regression model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = lr_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics (for AUC and accuracy)\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using precision\n","precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","precision = precision_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using recall\n","recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","recall = recall_evaluator.evaluate(predictions)\n","\n","# Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","                              .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# Confusion Matrix\n","prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","metrics = MulticlassMetrics(prediction_and_labels)\n","\n","accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# ROC Curve\n","roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# Plot Confusion Matrix as Heatmap\n","plt.figure(figsize=(8,6))\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","roc = lr_model.summary.roc.toPandas()\n","plt.figure()\n","plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"cHigGMxTz0zG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from pyspark.ml.classification import LogisticRegression\n","# from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","# from pyspark.mllib.evaluation import MulticlassMetrics\n","# from pyspark.ml.feature import StandardScaler\n","# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# import time\n","# from pyspark.sql import functions as F\n","\n","# # Scale the features\n","# scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n","# scaler_model = scaler.fit(train_set)\n","# train_set = scaler_model.transform(train_set)\n","# test_set = scaler_model.transform(test_set)\n","\n","# # Initialize a Logistic Regression model\n","# lr = LogisticRegression(featuresCol='scaled_features', labelCol='is_fraud')\n","\n","# # Hyperparameter tuning using Cross-Validation\n","# paramGrid = ParamGridBuilder() \\\n","#     .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n","#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n","#     .build()\n","\n","# crossval = CrossValidator(estimator=lr,\n","#                           estimatorParamMaps=paramGrid,\n","#                           evaluator=MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1'),\n","#                           numFolds=3)  # Use 3+ folds for better results\n","\n","# # Train the model\n","# start = time.time()\n","# cvModel = crossval.fit(train_set)\n","# end = time.time()\n","# print(f\"Time to train logistic regression model with CV: {end - start:.4f} seconds\")\n","\n","# # Make predictions on the test set\n","# start = time.time()\n","# predictions = cvModel.transform(test_set)\n","# end = time.time()\n","# print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# # Evaluate the model using binary classification metrics (for AUC and accuracy)\n","# binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","# accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# # Evaluate the model using F1 score\n","# multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","# f1_score = multi_evaluator.evaluate(predictions)\n","\n","# # Evaluate the model using precision\n","# precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","# precision = precision_evaluator.evaluate(predictions)\n","\n","# # Evaluate the model using recall\n","# recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","# recall = recall_evaluator.evaluate(predictions)\n","\n","# # Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","# predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","#                               .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# # Confusion Matrix\n","# prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","# metrics = MulticlassMetrics(prediction_and_labels)\n","\n","# accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","# accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","# confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# # ROC Curve\n","# roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","# roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# # Print metrics\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","# print(f\"F1 Score: {f1_score:.4f}\")\n","# print(f\"Precision: {precision:.4f}\")\n","# print(f\"Recall: {recall:.4f}\")\n","# print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# # Plot Confusion Matrix as Heatmap\n","# plt.figure(figsize=(8,6))\n","# sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","# plt.title(\"Confusion Matrix\")\n","# plt.xlabel(\"Predicted\")\n","# plt.ylabel(\"Actual\")\n","# plt.show()\n","\n","# print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# # Plot ROC curve\n","# roc = lr_model.summary.roc.toPandas()\n","# plt.figure()\n","# plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('ROC Curve')\n","# plt.legend(loc='best')\n","# plt.show()"],"metadata":{"id":"r5NqnWOi1tIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from pyspark.sql import functions as F\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import time\n","import matplotlib.pyplot as plt\n","\n","# Create a logistic regression model with default parameters\n","gbt = GBTClassifier(featuresCol='features', labelCol='is_fraud')\n","\n","# Train the model using the training set\n","start = time.time()\n","gbt_model = gbt.fit(train_set)\n","end = time.time()\n","print(f\"Time to train GBTClassifier model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = gbt_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics (for AUC and accuracy)\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using precision\n","precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","precision = precision_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using recall\n","recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","recall = recall_evaluator.evaluate(predictions)\n","\n","# Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","                              .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# Confusion Matrix\n","prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","metrics = MulticlassMetrics(prediction_and_labels)\n","\n","accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# ROC Curve\n","roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# Plot Confusion Matrix as Heatmap\n","plt.figure(figsize=(8,6))\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","roc = lr_model.summary.roc.toPandas()\n","plt.figure()\n","plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"F33Fbx4R-g4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from pyspark.sql import functions as F\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import time\n","import matplotlib.pyplot as plt\n","\n","# Create a logistic regression model with default parameters\n","rf = RandomForestClassifier(featuresCol='features', labelCol='is_fraud')\n","\n","# Train the model using the training set\n","start = time.time()\n","rf_model = rf.fit(train_set)\n","end = time.time()\n","print(f\"Time to train GBTClassifier model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = rf_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics (for AUC and accuracy)\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using precision\n","precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","precision = precision_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using recall\n","recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","recall = recall_evaluator.evaluate(predictions)\n","\n","# Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","                              .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# Confusion Matrix\n","prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","metrics = MulticlassMetrics(prediction_and_labels)\n","\n","accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# ROC Curve\n","roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# Plot Confusion Matrix as Heatmap\n","plt.figure(figsize=(8,6))\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","roc = lr_model.summary.roc.toPandas()\n","plt.figure()\n","plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"_jeUVghj-iVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from pyspark.sql import functions as F\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import time\n","import matplotlib.pyplot as plt\n","\n","# Create a logistic regression model with default parameters\n","dt = DecisionTreeClassifier(\n","    featuresCol='features',\n","    labelCol='is_fraud',\n","    maxDepth=5,\n","    maxBins=32,\n","    minInstancesPerNode=1,\n","    impurity='gini'\n",")\n","\n","# Train the model using the training set\n","start = time.time()\n","dt_model = dt.fit(train_set)\n","end = time.time()\n","print(f\"Time to train GBTClassifier model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = dt_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics (for AUC and accuracy)\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using precision\n","precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","precision = precision_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using recall\n","recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","recall = recall_evaluator.evaluate(predictions)\n","\n","# Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","                              .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# Confusion Matrix\n","prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","metrics = MulticlassMetrics(prediction_and_labels)\n","\n","accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# ROC Curve\n","roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# Plot Confusion Matrix as Heatmap\n","plt.figure(figsize=(8,6))\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","roc = lr_model.summary.roc.toPandas()\n","plt.figure()\n","plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"OSkLhfVK-lX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.classification import LinearSVC\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import time\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pyspark.sql import functions as F\n","\n","# Create a Linear SVM model with default parameters\n","svm = LinearSVC(\n","    featuresCol='features',\n","    labelCol='is_fraud',\n","    maxIter=100,  # Maximum number of iterations\n","    regParam=0.0  # Regularization parameter (can tune this)\n",")\n","\n","# Train the model using the training set\n","start = time.time()\n","svm_model = svm.fit(train_set)\n","end = time.time()\n","print(f\"Time to train SVM model: {end - start:.4f} seconds\")\n","\n","# Make predictions on the test set\n","start = time.time()\n","predictions = svm_model.transform(test_set)\n","end = time.time()\n","print(f\"Time to make predictions on test set: {end - start:.4f} seconds\")\n","\n","# Evaluate the model using binary classification metrics (for AUC and accuracy)\n","binary_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud')\n","accuracy = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n","\n","# Evaluate the model using F1 score\n","multi_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n","f1_score = multi_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using precision\n","precision_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n","precision = precision_evaluator.evaluate(predictions)\n","\n","# Evaluate the model using recall\n","recall_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedRecall')\n","recall = recall_evaluator.evaluate(predictions)\n","\n","# Cast 'prediction' and 'is_fraud' columns to double for compatibility with MulticlassMetrics\n","predictions_cast = predictions.withColumn('prediction', F.col('prediction').cast('double'))\\\n","                              .withColumn('is_fraud', F.col('is_fraud').cast('double'))\n","\n","# Confusion Matrix\n","prediction_and_labels = predictions_cast.select('prediction', 'is_fraud').rdd\n","metrics = MulticlassMetrics(prediction_and_labels)\n","\n","accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n","accuracy = accuracy_evaluator.evaluate(predictions)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# ROC Curve\n","roc_evaluator = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n","roc_auc = roc_evaluator.evaluate(predictions)\n","\n","# Print metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n","\n","# Plot Confusion Matrix as Heatmap\n","plt.figure(figsize=(8,6))\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","print(f\"Area Under ROC: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","roc = lr_model.summary.roc.toPandas()\n","plt.figure()\n","plt.plot(roc['FPR'], roc['TPR'], label=f'AUC = {roc_auc:.4f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"WJGWIsZr-m--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LVRMeKKt1l0Y"},"execution_count":null,"outputs":[]}]}